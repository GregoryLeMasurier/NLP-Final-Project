{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project - Gregory LeMasurier and Mojtaba Talaei Khoei\n",
    "\n",
    "Making the training file a jupyter notebook for the time being so I can easily debug it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (0.0.4)\n",
      "Requirement already satisfied: nltk in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (3.7)\n",
      "Requirement already satisfied: sentencepiece in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (0.1.96)\n",
      "Requirement already satisfied: numpy in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from rouge-score) (1.21.5)\n",
      "Requirement already satisfied: absl-py in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from rouge-score) (1.0.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from nltk) (2022.1.18)\n",
      "Requirement already satisfied: tqdm in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: joblib in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from click->nltk) (4.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.7.0)\n"
     ]
    }
   ],
   "source": [
    "# Install Dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install rouge-score nltk sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Imports\n",
    "import os\n",
    "import random\n",
    "\n",
    "import transformers\n",
    "from transformers import PegasusTokenizer, PegasusConfig\n",
    "from transformers import PegasusForConditionalGeneration\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import wandb\n",
    "from packaging import version\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from transformer_mt import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"Summarization\")\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "datasets.utils.logging.set_verbosity_warning()\n",
    "transformers.utils.logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE Metric\n",
    "rouge = datasets.load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_only = True\n",
    "\n",
    "dataset_name = 'cnn_dailymail'\n",
    "dataset_version = '3.0.0'\n",
    "wandb_project = \"PegasusSummarization\"\n",
    "output_dir = \"output_dir/\"\n",
    "device = 'cuda' if (torch.cuda.is_available() and not cpu_only) else 'cpu'\n",
    "\n",
    "if torch.cuda.is_available:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model_name = 'google/pegasus-xsum' \n",
    "tokenizer_name = 'google/pegasus-cnn_dailymail'\n",
    "seq_len = 512\n",
    "batch_size = 8\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 0.0\n",
    "num_train_epochs = 1\n",
    "lr_scheduler_type = \"linear\"\n",
    "num_warmup_steps = 0\n",
    "eval_every_steps = 5\n",
    "\n",
    "# Flag to make \n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logger.info(f\"Starting tokenizer training\")\n",
    "\n",
    "    logger.info(f\"Loading dataset\")\n",
    "\n",
    "    wandb.init(project=wandb_project) #Skipping config for now - will add back later\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    raw_datasets = load_dataset(dataset_name, dataset_version)\n",
    "\n",
    "    # Make a small dataset for proof of concept\n",
    "    if debug:\n",
    "        raw_datasets = utils.sample_small_debug_dataset(raw_datasets)\n",
    "\n",
    "    ## TOKENIZER\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    ## PRETRAINED MODEL\n",
    "    #The pegasus model is too large to test on a laptop, so load a small config for now\n",
    "    #model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "    config = PegasusConfig(\n",
    "            encoder_layers=2, \n",
    "            decoder_layers=2, \n",
    "            encoder_attention_heads=8, \n",
    "            decoder_attention_heads=8, \n",
    "            decoder_ffn_dim=1024, \n",
    "            encoder_ffn_dim=1024,\n",
    "            max_position_embeddings=seq_len,\n",
    "            vocab_size=tokenizer.vocab_size\n",
    "            )\n",
    "    model = PegasusForConditionalGeneration(config).to(device)\n",
    "\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        inputs = [ex for ex in examples['article']]\n",
    "        targets = [ex for ex in examples['highlights']]\n",
    "        model_inputs = tokenizer(inputs, max_length=seq_len, truncation=True)\n",
    "        model_inputs['labels'] = tokenizer(targets, max_length=seq_len, truncation=True)['input_ids']\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=8,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Tokenizing the dataset\",\n",
    "    )\n",
    "\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"validation\"] if \"validaion\" in tokenized_datasets else tokenized_datasets[\"test\"]\n",
    "\n",
    "    for index in random.sample(range(len(train_dataset)), 2):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "        logger.info(f\"Decoded input_ids: {tokenizer.decode(train_dataset[index]['input_ids'])}\")\n",
    "        logger.info(f\"Decoded labels: {tokenizer.decode(train_dataset[index]['labels'])}\")\n",
    "        logger.info(\"\\n\")\n",
    "\n",
    "    #collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer, max_length=seq_len, padding='max_length')\n",
    "    collator = transformers.DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, max_length=seq_len, padding='max_length')\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        shuffle=True, \n",
    "        collate_fn=collator, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, \n",
    "        collate_fn=collator, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "    \n",
    "    # Scheduler and math around the number of training steps.\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = transformers.get_scheduler(\n",
    "        name=lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_train_steps,\n",
    "    )\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {num_train_epochs}\")\n",
    "    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n",
    "    progress_bar = tqdm(range(max_train_steps))\n",
    "\n",
    "    # Log a pre-processed training example to make sure the pre-processing does not have bugs in it\n",
    "    # and we do not input garbage to our model.\n",
    "    batch = next(iter(train_dataloader))\n",
    "\n",
    "    #logger.info(\"Look at the data that we input into the model, check that it looks like what we expect.\")\n",
    "    #for index in random.sample(range(len(batch)), 2):\n",
    "    #    logger.info(f\"Decoded input_ids size: {len(batch['input_ids'][index])}\")\n",
    "    #    logger.info(f\"Decoded input_ids: {tokenizer.decode(batch['input_ids'][index])}\")\n",
    "    #    logger.info(f\"Decoded labels size: {len(batch['labels'][index])}\")\n",
    "    #    logger.info(f\"Decoded labels: {tokenizer.decode(batch['labels'][index])}\")\n",
    "    #    logger.info(\"\\n\")\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(num_train_epochs):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            out = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = out[\"loss\"]\n",
    "            logits = out[\"logits\"]\n",
    "            #TODO: Top K sampling of logits\n",
    "\n",
    "            #print(loss.item())\n",
    "            #print(\"\\n\\n\")\n",
    "            #print(logits)\n",
    "            #print(\"\\n\\n\")\n",
    "            #print(labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train_loss\": loss,\n",
    "                    \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                    \"epoch\": epoch,\n",
    "                },\n",
    "                step=global_step,\n",
    "            )\n",
    "\n",
    "            if (global_step % eval_every_steps == 0) or (global_step >= max_train_steps):\n",
    "                model.eval()\n",
    "\n",
    "                #TODO: USING SAME VALUE FOR PREDICTION AND REFERENCE!!!!\n",
    "                rouge_score = rouge.compute(predictions=labels, references=labels)\n",
    "\n",
    "                print(rouge_score)\n",
    "                print(\"\\n\\n\") \n",
    "\n",
    "\n",
    "                metric = {}\n",
    "                for rouge_type in rouge_score:\n",
    "                    print(rouge_score[rouge_type][0])\n",
    "                    print(\"\\n\\n\")\n",
    "                    metric['eval/' + rouge_type + \"/precision\"] = rouge_score[rouge_type][0][0]\n",
    "                    metric['eval/' + rouge_type + \"/recall\"] = rouge_score[rouge_type][0][1]\n",
    "                    metric['eval/' + rouge_type + \"/f1-score\"] = rouge_score[rouge_type][0][2]\n",
    "\n",
    "\n",
    "                print(metric)\n",
    "\n",
    "                wandb.log(metric, step=global_step)\n",
    "\n",
    "                logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "                model.save_pretrained(output_dir)\n",
    "\n",
    "                model.train()\n",
    "\n",
    "            if global_step >= max_train_steps:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2022 17:35:29 - INFO - Summarization - Starting tokenizer training\n",
      "04/19/2022 17:35:29 - INFO - Summarization - Loading dataset\n",
      "04/19/2022 17:35:29 - ERROR - wandb.jupyter - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mglemasurier\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/glemasurier/PegasusSummarization/runs/2mkg2qnc\" target=\"_blank\">wobbly-wood-17</a></strong> to <a href=\"https://wandb.ai/glemasurier/PegasusSummarization\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2022 17:35:33 - WARNING - datasets.builder - Reusing dataset cnn_dailymail (/home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "100%|██████████| 3/3 [00:00<00:00, 24.21it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Tokenizing the dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Tokenizing the dataset #3: 100%|██████████| 1/1 [00:00<00:00, 10.54ba/s]\n",
      "Tokenizing the dataset #1: 100%|██████████| 1/1 [00:00<00:00,  8.80ba/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Tokenizing the dataset #4: 100%|██████████| 1/1 [00:00<00:00,  8.44ba/s]\n",
      "Tokenizing the dataset #7: 100%|██████████| 1/1 [00:00<00:00, 10.19ba/s]\n",
      "Tokenizing the dataset #0: 100%|██████████| 1/1 [00:00<00:00,  8.48ba/s]\n",
      "\n",
      "Tokenizing the dataset #2: 100%|██████████| 1/1 [00:00<00:00,  9.31ba/s]\n",
      "Tokenizing the dataset #5: 100%|██████████| 1/1 [00:00<00:00, 11.45ba/s]\n",
      "Tokenizing the dataset #6: 100%|██████████| 1/1 [00:00<00:00, 12.01ba/s]\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-eb26f4d38f19f1f9.arrow\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-534063507fbf1abc.arrow\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-2c2b4b794d8708ee.arrow\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-741c28de26a69e47.arrow\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-c14eaf09d5675c12.arrow\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-86210058ba33eb2b.arrow\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-895c6668a17b77ce.arrow\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-001f1d8315b9f9d7.arrow\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-eb26f4d38f19f1f9.arrow\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-534063507fbf1abc.arrow\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-2c2b4b794d8708ee.arrow\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-741c28de26a69e47.arrow\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-c14eaf09d5675c12.arrow\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-86210058ba33eb2b.arrow\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-895c6668a17b77ce.arrow\n",
      "04/19/2022 17:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-001f1d8315b9f9d7.arrow\n",
      "04/19/2022 17:35:42 - INFO - Summarization - Sample 79 of the training set: {'input_ids': [143, 40155, 158, 1315, 3064, 131, 116, 6981, 554, 24315, 13184, 1191, 17212, 40722, 252, 1327, 122, 28189, 111, 50765, 116, 113, 14282, 111, 1503, 108, 22946, 3740, 112, 1290, 200, 135, 509, 686, 112, 114, 25154, 120, 117, 373, 4181, 113, 3064, 672, 111, 223, 113, 203, 1925, 604, 1836, 107, 139, 531, 131, 116, 765, 900, 118, 18243, 10620, 1668, 120, 1315, 645, 665, 198, 7244, 121, 37785, 194, 5662, 113, 114, 360, 713, 113, 336, 16192, 111, 1503, 1315, 186, 140, 198, 1321, 84628, 122, 181, 13055, 3453, 194, 279, 950, 114, 107, 208, 107, 11811, 114, 107, 208, 107, 10712, 250, 182, 140, 198, 79740, 141, 114, 59699, 51866, 74422, 1256, 122, 6568, 112, 423, 33667, 116, 108, 162, 148, 174, 5965, 141, 114, 4548, 15291, 113, 1256, 14282, 111, 336, 16192, 745, 992, 112, 109, 657, 2102, 107, 654, 120, 166, 108, 109, 5662, 196, 1871, 134, 205, 4087, 5020, 4653, 108, 7576, 1377, 158, 607, 6981, 554, 24315, 13184, 1191, 131, 116, 25154, 107, 9281, 196, 6852, 124, 109, 6211, 113, 1212, 29224, 718, 14595, 26880, 116, 111, 25846, 8959, 144, 16864, 554, 108, 130, 210, 130, 109, 4905, 297, 113, 109, 517, 113, 79010, 107, 398, 109, 242, 8278, 124, 108, 109, 17566, 1001, 34476, 254, 902, 107, 983, 2033, 657, 731, 134, 280, 891, 107, 208, 107, 1315, 339, 539, 244, 109, 211, 156, 687, 165, 1315, 5348, 120, 14282, 108, 5147, 111, 1503, 195, 73837, 142, 1077, 113, 2527, 5020, 164, 190, 109, 4006, 107, 6517, 3740, 7162, 489, 373, 665, 11294, 143, 26497, 1567, 158, 113, 109, 17212, 107, 6890, 115, 114, 1146, 1669, 16321, 113, 3064, 672, 108, 6981, 554, 24315, 13184, 1191, 137, 129, 684, 135, 186, 124, 114, 786, 242, 107, 168, 117, 156, 113, 3064, 131, 116, 1330, 12932, 111, 289, 196, 114, 698, 30738, 115, 13807, 343, 186, 133, 174, 53106, 116, 381, 237, 107, 2882, 960, 108, 118, 2468, 108, 4182, 5221, 114, 4548, 4762, 113, 336, 16192, 111, 6568, 3912, 113, 14282, 4220, 135, 109, 25154, 107, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [9281, 111, 336, 16192, 23196, 607, 3064, 131, 116, 6981, 554, 24315, 13184, 1191, 25154, 110, 107, 9281, 4786, 124, 500, 6211, 111, 972, 113, 109, 517, 113, 79010, 110, 107, 24643, 1585, 200, 135, 313, 373, 114, 12393, 9144, 13605, 113, 109, 17212, 110, 107, 651, 114, 786, 242, 108, 6981, 554, 24315, 13184, 1191, 137, 129, 684, 115, 3064, 672, 110, 107, 1]}.\n",
      "04/19/2022 17:35:42 - INFO - Summarization - Decoded input_ids: (CNN) -- Mexico's Popocatepetl volcano rumbled Saturday with explosions and expulsions of ash and gas, prompting authorities to bar people from getting close to a crater that is within sight of Mexico City and many of its 19 million residents. The country's National Center for Disaster Prevention reported that -- following 12 \"low-intensity\" emissions of a small amount of water vapor and gas -- there was \"an exhalation with some explosive component\" around 9 a.m. (10 a.m. ET). This was \"followed by a spasmodic tremor... with moderate to large amplitudes, which has been accompanied by a continuous emission of... ash and water vapor,\" according to the government agency. At that time, the emissions had gone at most 400 meters (1,300 feet) above Popocatepetl's crater. Ash had fallen on the towns of San Nicolas de los Ranchos and Huejotzingo, as well as the northern part of the city of Puebla. As the day wore on, the volcanic material soared even higher. An updated government report at 2 p.m. -- three hours after the first one went out -- indicated that ash, steam and gas were spewing an average of 500 meters up into the sky. Mexican authorities restricted access within 12 kilometers (7.5 miles) of the volcano. Located in a national park southeast of Mexico City, Popocatepetl can be seen from there on a clear day. It is one of Mexico's highest peaks and last had a major eruption in 2000. But there have been rumblings since then. Last April, for instance, scientists observed a continuous column of water vapor and moderate amounts of ash rising from the crater.\n",
      "04/19/2022 17:35:43 - INFO - Summarization - Decoded labels: Ash and water vapor soar above Mexico's Popocatepetl crater. Ash falls on several towns and parts of the city of Puebla. Authorities prevent people from going within a 7-mile radius of the volcano. On a clear day, Popocatepetl can be seen in Mexico City.\n",
      "04/19/2022 17:35:43 - INFO - Summarization - \n",
      "\n",
      "04/19/2022 17:35:43 - INFO - Summarization - Sample 62 of the training set: {'input_ids': [1741, 143, 40155, 158, 1315, 9809, 228, 121, 16850, 113, 6248, 131, 116, 5230, 1488, 7957, 7582, 120, 1319, 5801, 11886, 107, 25257, 20781, 138, 1023, 109, 449, 131, 116, 85076, 124, 1842, 108, 155, 169, 1541, 131, 116, 7711, 118, 2295, 204, 1824, 8122, 107, 7836, 2040, 117, 9785, 107, 398, 109, 2674, 44273, 279, 109, 449, 395, 153, 3977, 8221, 112, 114, 16434, 48404, 32880, 108, 11869, 14873, 5680, 6248, 12466, 32908, 1315, 330, 449, 18245, 108, 391, 4863, 111, 829, 2662, 108, 2123, 18406, 112, 5230, 11886, 107, 10551, 22312, 20035, 108, 260, 111, 7728, 820, 456, 1725, 108, 7014, 113, 1331, 48404, 4515, 108, 111, 114, 809, 113, 176, 829, 10162, 1315, 112, 4676, 109, 1674, 113, 109, 12466, 1580, 107, 222, 109, 11869, 6248, 12466, 32908, 2629, 108, 114, 1907, 2198, 113, 109, 5680, 5230, 1488, 7957, 666, 20781, 192, 1365, 211, 204, 2040, 107, 2632, 138, 1365, 211, 152, 4931, 25257, 20781, 151, 26528, 5179, 143, 85503, 158, 4931, 7836, 2040, 151, 26528, 6113, 5881, 34082, 4931, 28287, 73228, 151, 26528, 740, 143, 32693, 4931, 351, 144, 76585, 151, 26528, 740, 143, 32693, 4931, 7888, 10359, 151, 26528, 740, 143, 32693, 4931, 7888, 30770, 12033, 151, 26528, 3142, 143, 13795, 158, 4931, 6962, 686, 112, 443, 191, 27396, 1206, 151, 26528, 914, 143, 24742, 158, 4123, 176, 6248, 12466, 32908, 666, 109, 1580, 140, 188, 314, 686, 112, 443, 317, 20781, 111, 2040, 108, 111, 157, 3093, 274, 228, 112, 1365, 115, 114, 2609, 1206, 107, 325, 372, 9198, 114, 541, 121, 3018, 1619, 8150, 118, 211, 317, 20781, 108, 2040, 108, 1319, 1087, 12366, 351, 144, 76585, 111, 1319, 4900, 8271, 107, 7888, 30770, 12033, 107, 1041, 30770, 12033, 148, 174, 42217, 115, 109, 12248, 115, 109, 976, 396, 113, 24880, 108, 146, 114, 612, 6248, 12466, 17770, 3093, 342, 112, 129, 109, 15412, 2872, 124, 48404, 565, 107, 398, 156, 6248, 5230, 3436, 108, 20781, 148, 550, 114, 53240, 420, 1541, 197, 178, 368, 115, 169, 3390, 48404, 1441, 108, 173, 178, 6488, 1124, 3912, 113, 166, 111, 1040, 115, 109, 449, 6225, 114, 3669, 108, 209, 112, 236, 1319, 9309, 11886, 107, 3410, 74001, 8418, 1723, 121, 18544, 111, 28847, 5567, 118, 109, 1023, 107, 198, 1097, 148, 550, 114, 2103, 1541, 206, 146, 109, 1541, 120, 6248, 37370, 3106, 192, 4213, 172, 112, 236, 108, 155, 109, 2148, 148, 479, 109, 268, 474, 115, 6248, 135, 143, 61461, 158, 2772, 112, 3885, 112, 1477, 6697, 112, 462, 264, 745, 156, 12466, 17770, 108, 170, 1049, 120, 169, 442, 146, 129, 263, 108, 243, 113, 20781, 107, 198, 567, 875, 131, 144, 1158, 160, 505, 117, 109, 564, 113, 109, 166, 178, 414, 115, 6248, 541, 231, 754, 111, 199, 178, 309, 148, 648, 111, 142, 2248, 3561, 113, 1034, 15982, 3681, 131, 279, 109, 449, 107, 285, 148, 196, 109, 408, 111, 239, 119, 127, 1215, 112, 236, 181, 7759, 134, 169, 702, 107, 285, 4777, 114, 686, 48404, 496, 8801, 108, 173, 274, 310, 5680, 6248, 12466, 32908, 195, 1049, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [11869, 14873, 5680, 6248, 12466, 1488, 32908, 390, 269, 109, 3246, 85076, 110, 107, 2508, 121, 16850, 416, 120, 25257, 20781, 138, 1023, 124, 1842, 110, 107, 20834, 116, 163, 393, 7836, 2040, 138, 1365, 115, 453, 295, 110, 107, 1]}.\n",
      "04/19/2022 17:35:43 - INFO - Summarization - Decoded input_ids: Washington (CNN) -- Almost two-thirds of Iowa's Republican political pros predict that former Massachusetts Gov. Mitt Romney will win the state's caucuses on Tuesday, but his campaign's margin for error over Texas Rep. Ron Paul is slim. As the candidates dashed around the state making their closing arguments to a volatile caucus electorate, CNN surveyed 64 Iowa GOP insiders -- including state legislators, local elected and party officials, senior advisers to Republican Gov. Terry Branstad, business and conservative interest group leaders, veterans of previous caucus campaigns, and a variety of other party activists -- to assess the shape of the GOP race. In the CNN Iowa GOP insiders survey, a solid majority of the 64 Republican political pros thought Romney would finish first over Paul. Who will finish first? • Mitt Romney:.....40 (63%) • Ron Paul:.....17 (27%) • Michele Bachmann:.....1 (1%) • Newt Gingrich:.....1 (1%) • Rick Perry:.....1 (1%) • Rick Santorum:.....0 (0%) • Too close to call/dead heat:.....4 (6%) Three other Iowa GOP insiders thought the race was just too close to call between Romney and Paul, and they picked those two to finish in a dead heat. And another predicted a four-way traffic jam for first between Romney, Paul, former House Speaker Newt Gingrich and former Pennsylvania Sen. Rick Santorum. While Santorum has been surging in the polls in the final week of campaigning, not a single Iowa GOP insider picked him to be the outright winner on caucus night. As one Iowa Republican explained, Romney has run a shrewder campaign than he did in his 2008 caucus effort, when he bet huge amounts of time and resources in the state expecting a victory, only to see former Arkansas Gov. Mike Huckabee rally born-again and evangelical voters for the win. \"He has run a smart campaign; not the campaign that Iowa politicos would necessarily like to see, but the operation has done the right thing in Iowa from (managing) expectations to ads to finally deciding to play here,\" one GOP insider, who asked that his name not be used, said of Romney. \"What isn't written about enough is the value of the time he put in Iowa four years ago and how he still has staff and an extensive array of'super volunteers' around the state. He has had the money and now you are starting to see some enthusiasm at his events. He wins a close caucus.\" Indeed, when those same 64 Iowa GOP insiders were asked\n",
      "04/19/2022 17:35:43 - INFO - Summarization - Decoded labels: CNN surveyed 64 Iowa GOP political insiders days before the historic caucuses. Two-thirds say that Mitt Romney will win on Tuesday. Insiders also feel Ron Paul will finish in second place.\n",
      "04/19/2022 17:35:43 - INFO - Summarization - \n",
      "\n",
      "04/19/2022 17:35:43 - INFO - Summarization - ***** Running training *****\n",
      "04/19/2022 17:35:43 - INFO - Summarization -   Num examples = 100\n",
      "04/19/2022 17:35:43 - INFO - Summarization -   Num Epochs = 1\n",
      "04/19/2022 17:35:43 - INFO - Summarization -   Total optimization steps = 13\n",
      " 38%|███▊      | 5/13 [00:20<00:32,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)), 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)), 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)), 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}\n",
      "\n",
      "\n",
      "\n",
      "Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b9fb469b2a2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1.18.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This script requires Datasets 1.18.0 or higher. Please update via pip install -U datasets.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-0221104de2fd>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouge_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrouge_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                     \u001b[0mmetric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrouge_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/precision\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrouge_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrouge_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"p\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                     \u001b[0mmetric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrouge_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/recall\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrouge_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrouge_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                     \u001b[0mmetric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrouge_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/f1-score\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrouge_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrouge_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "    if version.parse(datasets.__version__) < version.parse(\"1.18.0\"):\n",
    "        raise RuntimeError(\"This script requires Datasets 1.18.0 or higher. Please update via pip install -U datasets.\")\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78a98554bd959fe647588642884065a24bb8267d1904c1c950aa6b68cc3632ad"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('nlp_class')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
