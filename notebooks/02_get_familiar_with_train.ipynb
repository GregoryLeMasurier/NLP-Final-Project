{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append(\"../cli\")  # make all python files from the directory cli/ importable\n",
    "import train as train_script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training script\n",
    "\n",
    "Every training script roughly follows the same idea:\n",
    "\n",
    "1. Load the data\n",
    "2. Create the model and load the tokenizers\n",
    "3. Pre-process the data\n",
    "4. Create PyTorch dataloaders that handle data shuffling and batching\n",
    "5. Create optimizer and (optionally) learning rate scheduler\n",
    "6. Training loop\n",
    "7. Evaluation loop\n",
    "8. Save the model\n",
    "\n",
    "Some of these steps are usually simple (e.g., saving the model), some are usually complicated (e.g., pre-processing the data), but many of them really depends on what you are trying to achieve. For example, in most cases, the training loop is very standard, but as soon as you want to control your training more it becomes more and more complicated. The usual things you may want to add to training are early stopping, multi-GPU support, or just more metrics.\n",
    "\n",
    "Nevertheless, you will always see all of these steps in every deep learning project, so let's look at them closer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data\n",
    "\n",
    "In our case, it is a very simple task. We use Datasets library, which downloads the data files from the hub and provides us with a `Dataset` object. You can also use the same `load_dataset` to load your local files (not datasets form the hub) in `.csv` or `.json` formats.\n",
    "\n",
    "This is what this part looks like in our training script. Note that we check if the dataset has a validation split, and if it doesn't we create one.\n",
    "\n",
    "```python\n",
    "\n",
    "    raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n",
    "    if \"validation\" not in raw_datasets:\n",
    "        # will create \"train\" and \"test\" subsets\n",
    "        # fix seed to make sure that the split is reproducible\n",
    "        # note that we should use the same seed here and in create_tokenizer.py\n",
    "        raw_datasets = raw_datasets[\"train\"].train_test_split(test_size=2000, seed=42)\n",
    "```\n",
    "\n",
    "Additionally, when developing the model, it can be very useful to work not with the whole dataset, but with a very small sample of it (maybe just 100 examples). This way your training loop will be quick and you can quickly iterate on your code and fix all the bugs.\n",
    "\n",
    "```python\n",
    "    if args.debug:\n",
    "        raw_datasets = utils.sample_small_debug_dataset(raw_datasets)\n",
    "```\n",
    "\n",
    "You can find the function `sample_small_debug_dataset()` in `transformer_mt/utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = True\n",
    "assert done, \"please read the instructions above and then change the value of done to True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the model and load the tokenizers\n",
    "\n",
    "Here we create `PreTrainedTokenizerFast` and `TransfomerEncoderDecoderModel` objects that we will use throughout the rest of the script. Because this is a machine translation task, we have two tokenizers: one for the language we are translating **from** and the other for the language we are translating **to**.\n",
    "\n",
    "```python\n",
    "    source_tokenizer = ...\n",
    "    source_tokenizer = ...\n",
    "    \n",
    "    model = TransfomerEncoderDecoderModel(...)\n",
    "```\n",
    "\n",
    "> Now, go to `train.py` and complete tasks 4.1 and 4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = True\n",
    "assert done, \"please complete the task described above and then change the value of done to True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-process the data\n",
    "\n",
    "This is usually a very annoying to code part of the script. This is why we did it for you!\n",
    "\n",
    "For the translation task, we need to\n",
    "1. Tokenize source and target texts with the corresponding tokenizer.\n",
    "1. Truncate them to the maximum length we want our model to work with.\n",
    "1. Shift decoder inputs to the left from the target values, so that decoder would learn to predict the next word of the translation given previous words.\n",
    "\n",
    "\n",
    "> Now, answer inline questions 4.1 and 4.2 in `preprocess_function()`. Please write your answers in `train.py` and **not** in this notebook.\n",
    "\n",
    "Try out `preprocess_function` and feel free to play with its input. Notice that `decoder_input_ids` are almost exactly like `labels`, but have a special beginning-of-sentence token at the first position. During training, we always know the translation in advance and we train our system just like a language model. But when the model is trained and we translate a sentence without knowing its translation in advance, we always input the beginning-of-sentence (BOS) token into the decoder to produce the first word of the translation. Then we input `BOS` `first_word` to produce a second word, we input `BOS` `first_word` `second_word` to produce the third word, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1], [1, 1, 1, 1, 1, 1]],\n",
      " 'decoder_input_ids': [[119547, 101, 30120, 98214, 10129, 102],\n",
      "                       [119547,\n",
      "                        101,\n",
      "                        105415,\n",
      "                        10968,\n",
      "                        10305,\n",
      "                        118,\n",
      "                        24931,\n",
      "                        136,\n",
      "                        102]],\n",
      " 'input_ids': [[101, 31178, 102], [101, 14962, 10301, 13028, 136, 102]],\n",
      " 'labels': [[101, 30120, 98214, 10129, 102, 119548],\n",
      "            [101, 105415, 10968, 10305, 118, 24931, 136, 102, 119548]],\n",
      " 'token_type_ids': [[0, 0, 0], [0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "examples = {\"translation\": [\n",
    "    {\"en\": \"Hello\", \"fr\": \"Bonjour\"},\n",
    "    {\"en\": \"How are you?\", \"fr\": \"Comment allez-vous?\"},\n",
    "]}\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "max_seq_length = 128\n",
    "\n",
    "# ignore that we use the same tokeinzer for both languages, this is just for demonstration\n",
    "source_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "target_tokenizer.add_special_tokens({\"bos_token\": \"<bos>\"})\n",
    "target_tokenizer.add_special_tokens({\"eos_token\": \"<eos>\"})\n",
    "\n",
    "preprocessed = train_script.preprocess_function(\n",
    "    examples=examples,\n",
    "    source_lang=source_lang,\n",
    "    target_lang=target_lang,\n",
    "    source_tokenizer=source_tokenizer,\n",
    "    target_tokenizer=target_tokenizer,\n",
    "    max_seq_length=max_seq_length\n",
    ")\n",
    "pprint(preprocessed)\n",
    "\n",
    "done = True\n",
    "assert done, \"please complete the task described above and then change the value of done to True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create PyTorch dataloaders that handle data shuffling and batching\n",
    "\n",
    "When the data is pre-processed we need to be able to collate it into batches and we need to do it quickly.\n",
    "The problem is that any non-GPU computation you do during training slows you down. We don't want our GPUs to wait until the data is ready and this is why PyTorch provides us with DataLoaders, which can use multiple CPU cores to quickly read the data, (optionally) pre-process it, pad it to a fixed length, and combine into a batch.\n",
    "\n",
    "You can read more about dataloaders in the [official PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "Padding functions can be annoying to write too because there are so many ways to do them. For this homework we wrote a relatively inefficient, but easy-to-understand padding function `utils.pad()` and a function `collation_function_for_seq2seq()` that accepts a list of pre-processed examples, pads them to the maximum length of these examples and creates batches.\n",
    "\n",
    "```python\n",
    "def collation_function_for_seq2seq(batch, source_pad_token_id, target_pad_token_id):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        batch: a list of dicts of numpy arrays with keys\n",
    "            input_ids\n",
    "            decoder_input_ids\n",
    "            labels\n",
    "    \"\"\"\n",
    "    input_ids_list = [ex[\"input_ids\"] for ex in batch]\n",
    "    decoder_input_ids_list = [ex[\"decoder_input_ids\"] for ex in batch]\n",
    "    labels_list = [ex[\"labels\"] for ex in batch]\n",
    "\n",
    "    collated_batch = {\n",
    "        \"input_ids\": utils.pad(input_ids_list, source_pad_token_id),\n",
    "        \"decoder_input_ids\": utils.pad(decoder_input_ids_list, target_pad_token_id),\n",
    "        \"labels\": utils.pad(labels_list, target_pad_token_id),\n",
    "    }\n",
    "\n",
    "    collated_batch[\"encoder_padding_mask\"] = collated_batch[\"input_ids\"] == source_pad_token_id\n",
    "    return collated_batch\n",
    "```\n",
    "\n",
    "> Your next step is to code task 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decoder_input_ids': tensor([[119547,    101,  30120,  98214,  10129,    102,      1,      1,      1],\n",
      "        [119547,    101, 105415,  10968,  10305,    118,  24931,    136,    102]]),\n",
      " 'encoder_padding_mask': tensor([[False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False]]),\n",
      " 'input_ids': tensor([[  101, 31178,   102,     0,     0,     0],\n",
      "        [  101, 14962, 10301, 13028,   136,   102]]),\n",
      " 'labels': tensor([[   101,  30120,  98214,  10129,    102, 119548,      1,      1,      1],\n",
      "        [   101, 105415,  10968,  10305,    118,  24931,    136,    102, 119548]])}\n"
     ]
    }
   ],
   "source": [
    "batch = [\n",
    "    {\"input_ids\": [101, 31178, 102], \"decoder_input_ids\": [119547, 101, 30120, 98214, 10129, 102], \"labels\": [101, 30120, 98214, 10129, 102, 119548]},\n",
    "    {\"input_ids\": [101, 14962, 10301, 13028, 136, 102], \"decoder_input_ids\": [119547, 101, 105415, 10968, 10305, 118, 24931, 136, 102], \"labels\": [101, 105415, 10968, 10305, 118, 24931, 136, 102, 119548]},\n",
    "]\n",
    "collated_batch = train_script.collation_function_for_seq2seq(\n",
    "    batch,\n",
    "    source_pad_token_id=0,\n",
    "    target_pad_token_id=1,\n",
    ")\n",
    "\n",
    "pprint(collated_batch)\n",
    "\n",
    "done = True\n",
    "assert done, \"please complete the task described above and then change the value of done to True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create optimizer and (optionally) learning rate scheduler\n",
    "\n",
    "Usually a straightforward step.\n",
    "If you are using something simple like ADAM,\n",
    "just pass model parameters, learning rate, and any other extra arguments.\n",
    "\n",
    "```python\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "    )\n",
    "```\n",
    "\n",
    "A scheduler is an optional thing, but they work really well with Transformers.\n",
    "You can read more about schedulers here\n",
    "https://huggingface.co/docs/transformers/main_classes/optimizer_schedules\n",
    "transformers.get_scheduler is a convenience function that accepts\n",
    "the scheduler name and returns a function that changes the optimizer learning rate\n",
    "according to the schedule every time we call lr_scheduler.step()\n",
    "\n",
    "```python\n",
    "    lr_scheduler = transformers.get_scheduler(\n",
    "        name=args.lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.num_warmup_steps,\n",
    "        num_training_steps=args.max_train_steps,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = True\n",
    "assert done, \"please read the instructions above and then change the value of done to True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training loop\n",
    "\n",
    "Training loops can be arbitrary complex, but if we stick to the simplest one for our task, it would look roughly like this.\n",
    "\n",
    "```python\n",
    "    for epoch in range(args.num_train_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(args.device)\n",
    "            decoder_input_ids = batch[\"decoder_input_ids\"].to(args.device)\n",
    "            key_padding_mask = batch[\"encoder_padding_mask\"].to(args.device)\n",
    "            labels = batch[\"labels\"].to(args.device)\n",
    "\n",
    "            logits = model(\n",
    "                input_ids,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                key_padding_mask=key_padding_mask,\n",
    "            )\n",
    "\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.shape[-1]),\n",
    "                labels.view(-1),\n",
    "                ignore_index=target_tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            wandb.log({\n",
    "                \"train_loss\": loss,\n",
    "                \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                \"epoch\": epoch,\n",
    "                },\n",
    "                step=global_step,\n",
    "            )\n",
    "\n",
    "```\n",
    "\n",
    "Here, every batch is an object returned by a `collate_fn` function. \n",
    "\n",
    "1. Move all of the tensors you will use for your neural network input and loss calculation to the GPU.\n",
    "2. Produce logits with your model\n",
    "Remember that sequence-to-sequence model accepts\n",
    "  * input_ids (encoder input, the sequence we want to translate)\n",
    "  * decoder_input_ids (decoder input, the translation shifted to the left)\n",
    "  * key_padding_mask (for masking out PAD tokens in encoder input)\n",
    "3. Use F.cross_entropy to compute the loss.\n",
    "Notice that you might need to reshape the tensors to do that\n",
    "into [batch_size * sequence_length, vocab_size]\n",
    "and reshape labels into [batch_size * sequence_length]\n",
    "Ignore target_tokenizer.pad_token_id in loss computation (argument ignore_index).\n",
    "4. Compute the loss gradients with .backward()\n",
    "5. Update the parameters\n",
    "6. Update the learning rate using the scheduler\n",
    "7. Zero out the gradients so that they don't accumulate between steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = True\n",
    "assert done, \"please read the instructions above and then change the value of done to True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation loop\n",
    "\n",
    "Usually, evaluation is either performed at the end of an epoch or every n training iterations.\n",
    "The second approach is preferred if you have a very large training set for which one epoch can take hours\n",
    "(as we have in this homework).\n",
    "\n",
    "This is what it schematically looks like:\n",
    "\n",
    "```python\n",
    "    global_step = 0\n",
    "    for epoch in range(args.num_train_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            # training loop stuff\n",
    "            ...\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % args.eval_every_steps == 0 or global_step == args.max_train_steps:\n",
    "                eval_results, decoded_preds, decoded_labels = evaluate_model(\n",
    "                    model=model,\n",
    "                    eval_dataloader=eval_dataloader,\n",
    "                    ...\n",
    "                )\n",
    "                wandb.log(eval_results)\n",
    "```\n",
    "\n",
    "Evaluation of sequence-to-sequence models is quite different from the usual evaluation loop.\n",
    "Compared to the classification task, where you just need to produce a single number (class index) given the input,\n",
    "for generation tasks, you need to produce a sequence of numbers (token indices).\n",
    "\n",
    "And if during training we know them all in advance and insert `decoder_input_ids`,\n",
    "this is not how we want (or can) use these models in real life.\n",
    "We talked about greedy generation and beam search in the class,\n",
    "you can find their implementations in\n",
    "`transformer_mt/modeling_transformer.py`.\n",
    "Specially `TransfomerEncoderDecoderModel._generate_greedy`\n",
    "and `TransfomerEncoderDecoderModel._generate_beam_search`.\n",
    "\n",
    "As you may see, these functions are quite different in complexity,\n",
    "but the results of writing the beam search are rewarding. Sometimes the difference can be as large as 5 BLEU points.\n",
    "Feel free to compare your model performance with `beam_size=5` and `beam_size=1` (greedy generation).\n",
    "\n",
    "> Look into `evaluate_model()` in `train.py` and answer an inline question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the model\n",
    "\n",
    "To save the model, we need to save two entities: model weights which are also called, a checkpoint or a `state_dict`, and model config. We need this because to restore the model we need to first create a model object (with any parameters) and then use `.load_state_dict()` to replace the existing weights with the ones from the checkpoint.\n",
    "\n",
    "```python\n",
    "    logger.info(\"Saving final model checkpoint to %s\", args.output_dir)\n",
    "    model.save_pretrained(args.output_dir)\n",
    "\n",
    "    logger.info(\"Uploading tokenizer, model and config to wandb\")\n",
    "    wandb.save(os.path.join(args.output_dir, \"*\"))\n",
    "```\n",
    "\n",
    "Additionally, we upload everything to WandB, so that you could download a trained model from the website later if you need it and didn't have to store it on your machine or in the cloud.\n",
    "\n",
    "To see how `.save_pretrained()` and `.from_pretrained()` are implemented in this homework (which is significantly more simple compared to Transformers implementation of similar functions), feel free to look at `transformer_mt/modeling_transformer.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f86d5ac646110a331c60478f9400a1a64d0cd523be90d887457228f9723636b5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('nlp_class')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
