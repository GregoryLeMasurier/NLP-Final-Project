{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project - Gregory LeMasurier and Mojtaba Talaei Khoei\n",
    "\n",
    "Making the training file a jupyter notebook for the time being so I can easily debug it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install rouge-score nltk sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Imports\n",
    "import os\n",
    "import random\n",
    "\n",
    "import transformers\n",
    "from transformers import PegasusTokenizer, PegasusConfig\n",
    "from transformers import PegasusForConditionalGeneration\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import wandb\n",
    "from packaging import version\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from transformer_mt import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"Summarization\")\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "datasets.utils.logging.set_verbosity_warning()\n",
    "transformers.utils.logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE Metric\n",
    "rouge = datasets.load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_only = True\n",
    "\n",
    "dataset_name = 'cnn_dailymail'\n",
    "dataset_version = '3.0.0'\n",
    "wandb_project = \"PegasusSummarization\"\n",
    "output_dir = \"output_dir/\"\n",
    "device = 'cuda' if (torch.cuda.is_available() and not cpu_only) else 'cpu'\n",
    "\n",
    "if torch.cuda.is_available:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model_name = 'google/pegasus-xsum' \n",
    "tokenizer_name = 'google/pegasus-cnn_dailymail'\n",
    "seq_len = 1024\n",
    "batch_size = 8\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 0.0\n",
    "num_train_epochs = 10\n",
    "lr_scheduler_type = \"linear\"\n",
    "num_warmup_steps = 0\n",
    "eval_every_steps = 2000\n",
    "k = int(seq_len * 0.3)\n",
    "\n",
    "# Flag to make \n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logger.info(f\"Starting tokenizer training\")\n",
    "\n",
    "    logger.info(f\"Loading dataset\")\n",
    "\n",
    "    wandb.init(project=wandb_project) #Skipping config for now - will add back later\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    raw_datasets = load_dataset(dataset_name, dataset_version)\n",
    "\n",
    "    # Make a small dataset for proof of concept\n",
    "    if debug:\n",
    "        raw_datasets = utils.sample_small_debug_dataset(raw_datasets)\n",
    "\n",
    "    ## TOKENIZER\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    ## PRETRAINED MODEL\n",
    "    #The pegasus model is too large to test on a laptop, so load a small config for now\n",
    "    #model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "    config = PegasusConfig(\n",
    "            encoder_layers=2, \n",
    "            decoder_layers=2, \n",
    "            encoder_attention_heads=8, \n",
    "            decoder_attention_heads=8, \n",
    "            decoder_ffn_dim=1024, \n",
    "            encoder_ffn_dim=1024,\n",
    "            max_position_embeddings=seq_len,\n",
    "            vocab_size=tokenizer.vocab_size\n",
    "            )\n",
    "    model = PegasusForConditionalGeneration(config).to(device)\n",
    "\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        inputs = [ex for ex in examples['article']]\n",
    "        targets = [ex for ex in examples['highlights']]\n",
    "        model_inputs = tokenizer(inputs, max_length=seq_len, truncation=True)\n",
    "        model_inputs['labels'] = tokenizer(targets, max_length=seq_len, truncation=True)['input_ids']\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=8,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Tokenizing the dataset\",\n",
    "    )\n",
    "\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"validation\"] if \"validaion\" in tokenized_datasets else tokenized_datasets[\"test\"]\n",
    "    test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "\n",
    "    for index in random.sample(range(len(train_dataset)), 2):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "        logger.info(f\"Sample {index} of the training set input ids: {train_dataset[index]['input_ids']}.\")\n",
    "        logger.info(f\"Decoded input_ids: {tokenizer.decode(train_dataset[index]['input_ids'])}\")\n",
    "        logger.info(f\"Decoded labels: {tokenizer.decode(train_dataset[index]['labels'])}\")\n",
    "        logger.info(\"\\n\")\n",
    "\n",
    "    collator = transformers.DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, max_length=seq_len, padding='max_length', label_pad_token_id=0)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        shuffle=True, \n",
    "        collate_fn=collator, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, \n",
    "        collate_fn=collator, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, \n",
    "        collate_fn=collator, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "    \n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = transformers.get_scheduler(\n",
    "        name=lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_train_steps,\n",
    "    )\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {num_train_epochs}\")\n",
    "    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n",
    "    progress_bar = tqdm(range(max_train_steps))\n",
    "\n",
    "    batch = next(iter(train_dataloader))\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(num_train_epochs):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            out = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = out[\"loss\"]\n",
    "            logits = out[\"logits\"]\n",
    "            res = torch.topk(logits, k=k)\n",
    "            values = res[0]\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train_loss\": loss,\n",
    "                    \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                    \"epoch\": epoch,\n",
    "                },\n",
    "                step=global_step,\n",
    "            )\n",
    "\n",
    "            if (global_step % eval_every_steps == 0) or (global_step >= max_train_steps):\n",
    "                model.eval()\n",
    "\n",
    "                generations = []\n",
    "                eval_labels = []\n",
    "                for batch in eval_dataloader:\n",
    "                    eval_input_ids = batch[\"input_ids\"].to(device)\n",
    "                    eval_labels.append(batch[\"labels\"].to(device))\n",
    "                    encoded_summary = model.generate(eval_input_ids)\n",
    "                    generations.append(encoded_summary)\n",
    "                    #print(generations)\n",
    "                    #decodings = tokenizer.batch_decode(encoded_summary, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "                    #print(\"Decoding: \" + str(decodings))\n",
    "\n",
    "                rouge_score = rouge.compute(predictions=generations, references=eval_labels)\n",
    "\n",
    "                metric = {}\n",
    "                for rouge_type in rouge_score:\n",
    "                    metric['eval/' + rouge_type + \"/precision\"] = rouge_score[rouge_type][0][0]\n",
    "                    metric['eval/' + rouge_type + \"/recall\"] = rouge_score[rouge_type][0][1]\n",
    "                    metric['eval/' + rouge_type + \"/f1-score\"] = rouge_score[rouge_type][0][2]\n",
    "\n",
    "                wandb.log(metric, step=global_step)\n",
    "\n",
    "                logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "                model.save_pretrained(output_dir)\n",
    "\n",
    "                model.train()\n",
    "\n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "    summaries = []\n",
    "    test_labels = []\n",
    "    for batch in test_dataloader:\n",
    "        test_input_ids = batch[\"input_ids\"].to(device)\n",
    "        test_labels.append(batch[\"labels\"].to(device))\n",
    "        test_encoded_summary = model.generate(test_input_ids)\n",
    "        summaries.append(test_encoded_summary)\n",
    "        decoded_summaries = tokenizer.batch_decode(test_encoded_summary, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        print(\"Summary: \" + str(decoded_summaries))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "    if version.parse(datasets.__version__) < version.parse(\"1.18.0\"):\n",
    "        raise RuntimeError(\"This script requires Datasets 1.18.0 or higher. Please update via pip install -U datasets.\")\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78a98554bd959fe647588642884065a24bb8267d1904c1c950aa6b68cc3632ad"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('nlp_class')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
