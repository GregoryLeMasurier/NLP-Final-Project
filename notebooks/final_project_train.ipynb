{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project - Gregory LeMasurier and Mojtaba Talaei Khoei\n",
    "\n",
    "Making the training file a jupyter notebook for the time being so I can easily debug it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (0.0.4)\n",
      "Requirement already satisfied: nltk in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (3.7)\n",
      "Requirement already satisfied: sentencepiece in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (0.1.96)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from rouge-score) (1.0.0)\n",
      "Requirement already satisfied: numpy in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from rouge-score) (1.21.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from nltk) (2022.1.18)\n",
      "Requirement already satisfied: joblib in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from click->nltk) (4.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.7.0)\n"
     ]
    }
   ],
   "source": [
    "# Install Dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install rouge-score nltk sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Common Imports\n",
    "import os\n",
    "import random\n",
    "\n",
    "import transformers\n",
    "from transformers import PegasusTokenizer, PegasusConfig\n",
    "from transformers import PegasusForConditionalGeneration\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import wandb\n",
    "from packaging import version\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from transformer_mt import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"Summarization\")\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "datasets.utils.logging.set_verbosity_warning()\n",
    "transformers.utils.logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE Metric\n",
    "rouge = datasets.load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_only = True\n",
    "\n",
    "dataset_name = 'cnn_dailymail'\n",
    "dataset_version = '3.0.0'\n",
    "wandb_project = \"PegasusSummarization\"\n",
    "output_dir = \"output_dir/\"\n",
    "device = 'cuda' if (torch.cuda.is_available() and not cpu_only) else 'cpu'\n",
    "\n",
    "if torch.cuda.is_available:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model_name = 'google/pegasus-xsum' \n",
    "tokenizer_name = 'google/pegasus-cnn_dailymail'\n",
    "seq_len = 512\n",
    "batch_size = 8\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 0.0\n",
    "num_train_epochs = 1\n",
    "lr_scheduler_type = \"linear\"\n",
    "num_warmup_steps = 0\n",
    "eval_every_steps = 5\n",
    "\n",
    "# Flag to make \n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logger.info(f\"Starting tokenizer training\")\n",
    "\n",
    "    logger.info(f\"Loading dataset\")\n",
    "\n",
    "    wandb.init(project=wandb_project) #Skipping config for now - will add back later\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    raw_datasets = load_dataset(dataset_name, dataset_version)\n",
    "\n",
    "    # Make a small dataset for proof of concept\n",
    "    if debug:\n",
    "        raw_datasets = utils.sample_small_debug_dataset(raw_datasets)\n",
    "\n",
    "    ## TOKENIZER\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    ## PRETRAINED MODEL\n",
    "    #The pegasus model is too large to test on a laptop, so load a small config for now\n",
    "    #model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "    config = PegasusConfig(\n",
    "            encoder_layers=2, \n",
    "            decoder_layers=2, \n",
    "            encoder_attention_heads=8, \n",
    "            decoder_attention_heads=8, \n",
    "            decoder_ffn_dim=1024, \n",
    "            encoder_ffn_dim=1024,\n",
    "            max_position_embeddings=seq_len,\n",
    "            vocab_size=tokenizer.vocab_size\n",
    "            )\n",
    "    model = PegasusForConditionalGeneration(config).to(device)\n",
    "\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        inputs = [ex for ex in examples['article']]\n",
    "        targets = [ex for ex in examples['highlights']]\n",
    "        model_inputs = tokenizer(inputs, max_length=seq_len, truncation=True)\n",
    "        model_inputs['labels'] = tokenizer(targets, max_length=seq_len, truncation=True)['input_ids']\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=8,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Tokenizing the dataset\",\n",
    "    )\n",
    "\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"validation\"] if \"validaion\" in tokenized_datasets else tokenized_datasets[\"test\"]\n",
    "\n",
    "    for index in random.sample(range(len(train_dataset)), 2):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "        logger.info(f\"Decoded input_ids: {tokenizer.decode(train_dataset[index]['input_ids'])}\")\n",
    "        logger.info(f\"Decoded labels: {tokenizer.decode(train_dataset[index]['labels'])}\")\n",
    "        logger.info(\"\\n\")\n",
    "\n",
    "    #collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer, max_length=seq_len, padding='max_length')\n",
    "    collator = transformers.DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, max_length=seq_len, padding='max_length')\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        shuffle=True, \n",
    "        collate_fn=collator, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, \n",
    "        collate_fn=collator, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "    \n",
    "    # Scheduler and math around the number of training steps.\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = transformers.get_scheduler(\n",
    "        name=lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_train_steps,\n",
    "    )\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {num_train_epochs}\")\n",
    "    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n",
    "    progress_bar = tqdm(range(max_train_steps))\n",
    "\n",
    "    # Log a pre-processed training example to make sure the pre-processing does not have bugs in it\n",
    "    # and we do not input garbage to our model.\n",
    "    batch = next(iter(train_dataloader))\n",
    "\n",
    "    #logger.info(\"Look at the data that we input into the model, check that it looks like what we expect.\")\n",
    "    #for index in random.sample(range(len(batch)), 2):\n",
    "    #    logger.info(f\"Decoded input_ids size: {len(batch['input_ids'][index])}\")\n",
    "    #    logger.info(f\"Decoded input_ids: {tokenizer.decode(batch['input_ids'][index])}\")\n",
    "    #    logger.info(f\"Decoded labels size: {len(batch['labels'][index])}\")\n",
    "    #    logger.info(f\"Decoded labels: {tokenizer.decode(batch['labels'][index])}\")\n",
    "    #    logger.info(\"\\n\")\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(num_train_epochs):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            out = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = out[\"loss\"]\n",
    "            logits = out[\"logits\"]\n",
    "            #TODO: Top K sampling of logits\n",
    "\n",
    "            #print(loss.item())\n",
    "            #print(\"\\n\\n\")\n",
    "            #print(logits)\n",
    "            #print(\"\\n\\n\")\n",
    "            #print(labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train_loss\": loss,\n",
    "                    \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                    \"epoch\": epoch,\n",
    "                },\n",
    "                step=global_step,\n",
    "            )\n",
    "\n",
    "            if (global_step % eval_every_steps == 0) or (global_step >= max_train_steps):\n",
    "                model.eval()\n",
    "\n",
    "                #TODO: USING SAME VALUE FOR PREDICTION AND REFERENCE!!!!\n",
    "                rouge_score = rouge.compute(predictions=labels, references=labels)\n",
    "\n",
    "                metric = {}\n",
    "                for rouge_type in rouge_score:\n",
    "                    metric['eval/' + rouge_type + \"/precision\"] = rouge_score[rouge_type][0][0]\n",
    "                    metric['eval/' + rouge_type + \"/recall\"] = rouge_score[rouge_type][0][1]\n",
    "                    metric['eval/' + rouge_type + \"/f1-score\"] = rouge_score[rouge_type][0][2]\n",
    "\n",
    "                wandb.log(metric, step=global_step)\n",
    "\n",
    "                logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "                model.save_pretrained(output_dir)\n",
    "\n",
    "                model.train()\n",
    "\n",
    "            if global_step >= max_train_steps:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2022 17:37:15 - INFO - Summarization - Starting tokenizer training\n",
      "04/19/2022 17:37:15 - INFO - Summarization - Loading dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2mkg2qnc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13152... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4bfa5ac54f41929aa3742341239cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>█▆▄▃▁</td></tr><tr><td>train_loss</td><td>█▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>learning_rate</td><td>3e-05</td></tr><tr><td>train_loss</td><td>10.50129</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">wobbly-wood-17</strong>: <a href=\"https://wandb.ai/glemasurier/PegasusSummarization/runs/2mkg2qnc\" target=\"_blank\">https://wandb.ai/glemasurier/PegasusSummarization/runs/2mkg2qnc</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220419_173529-2mkg2qnc/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2mkg2qnc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/glemasurier/PegasusSummarization/runs/2k44vnae\" target=\"_blank\">morning-cloud-18</a></strong> to <a href=\"https://wandb.ai/glemasurier/PegasusSummarization\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2022 17:37:21 - WARNING - datasets.builder - Reusing dataset cnn_dailymail (/home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "100%|██████████| 3/3 [00:00<00:00, 543.47it/s]\n",
      "\n",
      "\n",
      "Tokenizing the dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tokenizing the dataset #3: 100%|██████████| 1/1 [00:00<00:00, 10.06ba/s]\n",
      "Tokenizing the dataset #0: 100%|██████████| 1/1 [00:00<00:00,  8.88ba/s]\n",
      "Tokenizing the dataset #1: 100%|██████████| 1/1 [00:00<00:00,  9.11ba/s]\n",
      "\n",
      "Tokenizing the dataset #2: 100%|██████████| 1/1 [00:00<00:00,  8.77ba/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tokenizing the dataset #4: 100%|██████████| 1/1 [00:00<00:00,  8.62ba/s]\n",
      "Tokenizing the dataset #7: 100%|██████████| 1/1 [00:00<00:00,  7.99ba/s]\n",
      "Tokenizing the dataset #5: 100%|██████████| 1/1 [00:00<00:00, 11.82ba/s]\n",
      "Tokenizing the dataset #6: 100%|██████████| 1/1 [00:00<00:00, 10.99ba/s]\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-e83447dfa72cdffe.arrow\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-07da96a4ea652575.arrow\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-d7442768757c2594.arrow\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-1fe4396ba1893056.arrow\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-7ebccdc97e884d89.arrow\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-4a8afead31311ffd.arrow\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-7078b22af286aacf.arrow\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-8c4c416dc9d55239.arrow\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-e83447dfa72cdffe.arrow\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-07da96a4ea652575.arrow\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-d7442768757c2594.arrow\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-1fe4396ba1893056.arrow\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-7ebccdc97e884d89.arrow\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-4a8afead31311ffd.arrow\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-7078b22af286aacf.arrow\n",
      "04/19/2022 17:37:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-8c4c416dc9d55239.arrow\n",
      "04/19/2022 17:37:31 - INFO - Summarization - Sample 40 of the training set: {'input_ids': [1060, 110, 107, 15452, 9845, 118, 7543, 9772, 110, 107, 202, 37262, 1019, 121, 1623, 2199, 2442, 141, 2048, 5244, 28700, 20214, 114, 1701, 162, 3558, 1543, 374, 109, 1169, 6018, 4978, 148, 7677, 118, 925, 118, 21868, 55778, 11226, 316, 112, 1084, 39018, 108, 109, 22953, 113, 109, 7140, 108, 2048, 5244, 22052, 342, 112, 1701, 21868, 13185, 111, 377, 67412, 116, 233, 1092, 13664, 107, 18998, 9977, 380, 233, 112, 1188, 6276, 10669, 3897, 68385, 107, 139, 1701, 5443, 68385, 108, 114, 855, 113, 13623, 108, 112, 374, 109, 1087, 113, 68385, 233, 156, 113, 109, 205, 5160, 90444, 111, 748, 2671, 113, 109, 1195, 307, 7580, 107, 139, 2199, 108, 13073, 108, 2442, 141, 2048, 5244, 28700, 9231, 109, 1474, 113, 114, 1701, 113, 21868, 13185, 111, 377, 67412, 116, 112, 1188, 6276, 10669, 3897, 68385, 110, 107, 2048, 5244, 28700, 108, 13073, 108, 4571, 114, 21060, 113, 2780, 112, 225, 1070, 1098, 111, 263, 408, 135, 109, 2056, 13520, 110, 107, 1006, 1601, 108, 6381, 3203, 68385, 108, 237, 701, 1562, 109, 328, 9490, 111, 115, 738, 16483, 3271, 109, 3084, 4978, 115, 109, 672, 113, 1169, 233, 109, 531, 131, 116, 211, 1279, 2264, 111, 2056, 112, 7862, 946, 260, 115, 2159, 107, 139, 28078, 2021, 151, 1034, 18502, 68385, 113, 150, 326, 29404, 326, 113, 1169, 122, 176, 110, 10436, 2534, 5448, 326, 141, 16648, 22787, 47370, 118, 109, 1474, 112, 150, 326, 207, 113, 109, 110, 116, 73634, 113, 7651, 326, 4062, 7299, 1761, 8756, 772, 111, 2069, 110, 116, 9238, 60165, 772, 134, 114, 28788, 304, 10152, 7447, 1879, 14559, 107, 131, 29405, 141, 5244, 28700, 1034, 31244, 840, 131, 115, 738, 5517, 233, 541, 231, 190, 169, 49192, 1019, 14729, 233, 178, 163, 2869, 118, 109, 1474, 112, 129, 8918, 118, 228, 231, 1071, 2886, 334, 10242, 135, 68385, 109, 6665, 192, 129, 37268, 107, 654, 109, 166, 113, 109, 1701, 108, 34782, 1019, 121, 1623, 68385, 140, 114, 698, 6276, 10669, 2430, 115, 42204, 108, 8878, 107, 983, 1506, 9112, 1868, 108, 178, 1502, 130, 109, 2346, 6781, 113, 1169, 108, 114, 5617, 113, 7533, 111, 140, 114, 698, 69962, 112, 109, 6665, 107, 139, 2199, 117, 5145, 112, 1084, 39018, 108, 109, 22953, 113, 109, 7140, 107, 2048, 5244, 22052, 342, 112, 1701, 162, 108, 380, 108, 192, 129, 1092, 109, 4526, 113, 13664, 107, 18998, 9977, 110, 107, 139, 3084, 4978, 115, 109, 672, 113, 1169, 233, 109, 531, 131, 116, 211, 1279, 2264, 111, 2056, 112, 7862, 946, 260, 115, 2159, 110, 107, 139, 1701, 140, 297, 113, 114, 21060, 113, 35512, 5244, 4571, 112, 225, 1070, 1098, 111, 642, 135, 109, 2056, 13520, 107, 168, 1543, 2617, 68385, 131, 116, 13418, 111, 1029, 231, 678, 169, 1601, 6381, 3203, 140, 1723, 233, 170, 163, 1257, 2346, 6781, 113, 1169, 111, 1843, 2581, 124, 109, 1968, 117, 309, 1373, 115, 1169, 380, 107, 139, 47686, 121, 1019, 121, 1623, 76610, 2199, 233, 162, 495, 126, 117, 114, 17181, 266, 135, 20520, 769, 233, 559, 3373, 297, 113, 109, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [37262, 1019, 121, 1623, 2199, 2442, 141, 2048, 5244, 28700, 313, 124, 925, 118, 21868, 18852, 110, 107, 13016, 4571, 114, 1701, 162, 1543, 374, 109, 1169, 6018, 4978, 110, 107, 2048, 3478, 1701, 113, 21868, 13185, 112, 129, 1389, 112, 6276, 10669, 3897, 68385, 110, 107, 1006, 1601, 6381, 3203, 68385, 3271, 109, 3084, 4978, 115, 672, 113, 1169, 110, 107, 10179, 140, 4526, 113, 13664, 107, 18998, 208, 380, 111, 297, 113, 2048, 131, 116, 5469, 112, 3221, 1456, 110, 107, 1]}.\n",
      "04/19/2022 17:37:31 - INFO - Summarization - Decoded input_ids: By. Julian Robinson for MailOnline. A 500-year-old document signed by King Henry VIII granting a loan which ultimately helped found the London Stock Exchange has emerged for sale for £65,000. Addressed to John Heron, the Treasurer of the Chamber, King Henry informs him to loan £637 and 10 shillings - worth £3.94million today - to English cloth merchant Richard Gresham. The loan enabled Gresham, a member of parliament, to found the House of Gresham - one of the most celebrated mercantile and financial houses of the 16th Century. The document, pictured, signed by King Henry VIII outlined the payment of a loan of £637 and 10 shillings to English cloth merchant Richard Gresham. King Henry VIII, pictured, granted a raft of loans to help develop businesses and used money from the central purse. His son, Sir Thomas Gresham, then further increased the family fortune and in 1571 founded the Royal Exchange in the City of London - the country's first stock exchange and central to founding modern business in England. The monarch wrote: 'Richard Gresham of oure Citie of London with other stonde bounde by obligacion for the payment to oure use of the somme of Sixe hundred thirty seven poundes and ten shillynges at a certayn daie expired.' Signed by Henry VIII 'Henry R' in 1513 - four years into his 38-year reign - he also orders for the payment to be delayed for two years upon receiving sureties from Gresham the crown would be repaid. At the time of the loan, 29-year-old Gresham was a major cloth merchant trading in Antwerp, Belgium. An extremely influential figure, he served as the Lord Mayor of London, a Member of Parliament and was a major financier to the crown. The document is addressed to John Heron, the Treasurer of the Chamber. King Henry informs him to loan which, today, would be worth the equivalent of £3.94million. The Royal Exchange in the City of London - the country's first stock exchange and central to founding modern business in England. The loan was part of a raft of handouts Henry granted to help develop businesses and came from the central purse. It helped fund Gresham's empire and six years later his son Sir Thomas was born - who also became Lord Mayor of London and whose influence on the economy is still felt in London today. The 505-year-old vellum document - which means it is a parchment made from calf skin - once formed part of the\n",
      "04/19/2022 17:37:31 - INFO - Summarization - Decoded labels: 500-year-old document signed by King Henry VIII going on sale for £65,000. Document granted a loan which helped found the London Stock Exchange. King ordered loan of £637 to be paid to cloth merchant Richard Gresham. His son Sir Thomas Gresham founded the Royal Exchange in City of London. Loan was equivalent of £3.94m today and part of King's bid to boost trade.\n",
      "04/19/2022 17:37:31 - INFO - Summarization - \n",
      "\n",
      "04/19/2022 17:37:31 - INFO - Summarization - Sample 17 of the training set: {'input_ids': [143, 40155, 158, 1315, 11997, 13570, 24956, 1110, 124, 4360, 114, 17129, 552, 1315, 2425, 1732, 1315, 155, 126, 140, 11997, 1110, 120, 140, 24268, 938, 130, 351, 3477, 243, 126, 117, 4383, 108, 957, 960, 2628, 112, 2966, 3685, 121, 19172, 439, 835, 115, 109, 449, 107, 351, 3477, 117, 146, 1600, 107, 4599, 108, 5629, 108, 1824, 111, 3353, 163, 6033, 1443, 925, 113, 1732, 112, 2359, 107, 110, 72212, 108, 11997, 2792, 32981, 20248, 140, 7385, 111, 148, 3825, 200, 115, 109, 3512, 816, 112, 275, 112, 351, 859, 132, 4900, 112, 631, 11997, 1732, 107, 1478, 2686, 682, 11997, 148, 174, 165, 112, 795, 109, 439, 121, 24412, 278, 124, 203, 693, 108, 171, 429, 122, 109, 5812, 252, 10599, 327, 111, 411, 3689, 109, 230, 200, 115, 109, 475, 107, 283, 107, 631, 1732, 107, 168, 137, 1334, 120, 230, 130, 11997, 12768, 112, 376, 1924, 203, 1732, 1443, 112, 203, 527, 108, 155, 2987, 115, 109, 28118, 55627, 117, 120, 20248, 1847, 148, 243, 120, 130, 126, 6630, 108, 11997, 864, 192, 129, 383, 134, 5267, 203, 2210, 1315, 111, 835, 1315, 224, 5812, 252, 23642, 107, 1984, 108, 6842, 23642, 279, 109, 531, 5184, 290, 32930, 127, 1125, 1747, 120, 193, 439, 1783, 1294, 111, 1511, 114, 281, 476, 113, 639, 3303, 107, 20248, 256, 8469, 121, 22051, 8028, 170, 137, 1511, 109, 306, 178, 1728, 118, 169, 527, 108, 188, 130, 20162, 368, 173, 126, 2365, 203, 789, 115, 109, 475, 107, 283, 107, 115, 24431, 17093, 111, 10058, 1732, 117, 142, 3143, 291, 260, 135, 395, 183, 107, 3270, 5812, 252, 23642, 44448, 109, 439, 2845, 113, 109, 6550, 1863, 6469, 113, 2536, 118, 111, 14292, 5659, 121, 526, 121, 33774, 1631, 107, 485, 131, 116, 156, 870, 109, 2253, 503, 687, 122, 120, 861, 115, 109, 211, 295, 107, 6530, 108, 439, 23642, 127, 356, 1933, 3238, 108, 14382, 190, 109, 1146, 1968, 2490, 113, 3734, 113, 835, 121, 12073, 2729, 108, 11371, 113, 3734, 113, 2729, 115, 10427, 4532, 111, 13353, 113, 2729, 115, 49986, 107, 485, 493, 183, 2324, 14513, 107, 168, 163, 1106, 183, 53440, 120, 324, 7738, 245, 112, 1459, 107, 1439, 4365, 118, 439, 3451, 111, 439, 1721, 108, 114, 1146, 952, 113, 5812, 252, 23642, 122, 391, 835, 111, 292, 1631, 137, 193, 1783, 111, 5375, 118, 114, 177, 132, 263, 1143, 3074, 400, 107, 226, 137, 236, 447, 109, 641, 382, 3558, 3455, 112, 11997, 107, 11997, 1315, 130, 2030, 108, 291, 111, 17129, 130, 126, 218, 129, 1315, 117, 309, 114, 360, 1446, 115, 114, 221, 423, 8879, 107, 168, 1575, 188, 365, 22741, 1732, 289, 232, 6826, 107, 1912, 13570, 1575, 154, 197, 120, 290, 242, 107, 240, 11997, 148, 142, 1324, 124, 1225, 874, 108, 109, 1113, 10599, 861, 108, 115, 203, 205, 6842, 515, 108, 117, 114, 1936, 109, 789, 3756, 131, 144, 6477, 107, 1984, 108, 11997, 117, 14431, 120, 126, 148, 220, 1385, 5812, 252, 8028, 122, 162, 203, 3685, 835, 4681, 108, 111, 120, 126, 117, 1924, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [351, 3477, 131, 116, 177, 775, 493, 126, 4383, 118, 11997, 112, 1364, 1732, 1072, 112, 2359, 110, 107, 1084, 1141, 131, 47308, 151, 11997, 117, 146, 160, 112, 164, 2797, 109, 5812, 252, 10599, 327, 110, 107, 285, 649, 130, 1924, 111, 10058, 1732, 117, 142, 3143, 291, 260, 135, 395, 183, 110, 107, 1141, 131, 47308, 151, 398, 11997, 6630, 108, 126, 138, 163, 217, 112, 207, 109, 1113, 10599, 861, 110, 107, 1]}.\n",
      "04/19/2022 17:37:31 - INFO - Summarization - Decoded input_ids: (CNN) -- Tesla Motors prides itself on promoting a disruptive technology -- electric cars -- but it was Tesla itself that was disrupted recently as New Jersey said it is illegal, effective April 1, to operate factory-direct car sales in the state. New Jersey is not alone. Arizona, Maryland, Texas and Virginia also ban direct sale of cars to consumers. Understandably, Tesla CEO Elon Musk was upset and has encouraged people in the Garden State to go to New York or Pennsylvania to buy Tesla cars. Many wonder whether Tesla has been out to turn the car-marketing world on its head, do away with the franchised dealership system and change forever the way people in the U.S. buy cars. It can seem that way as Tesla fights to keep selling its cars direct to its customers, but missed in the speculative fervor is that Musk himself has said that as it grows, Tesla probably would be looking at expanding its presence -- and sales -- through franchised dealerships. Today, progressive dealerships around the country representing every automaker are creating experiences that make car buying easier and deliver a high level of customer satisfaction. Musk could cherry-pick dealers who can deliver the experience he wants for his customers, just as Lexus did when it launched its brand in the U.S. in 1989. Selling and servicing cars is an entirely different business from making them. Using franchised dealerships relieves the car manufacturer of the tremendous capital burden of paying for and staffing brick-and-mortar facilities. That's one reason the auto industry went with that model in the first place. Besides, car dealerships are important corporate citizens, pumping into the national economy hundreds of millions of sales-tax dollars, tens of millions of dollars in charitable contributions and billions of dollars in paychecks. That makes them valuable economically. It also gives them clout that few politicians want to challenge. More importantly for car buyers and car owners, a national network of franchised dealerships with local sales and service facilities can make buying and caring for a new or used vehicle relatively easy. You can see why the idea might ultimately appeal to Tesla. Tesla -- as innovative, different and disruptive as it may be -- is still a small player in a very large arena. It sold just under 25,000 cars last year globally. General Motors sold more than that every day. If Tesla has an eye on significant growth, the traditional dealership model, in its most progressive form, is a path the brand shouldn't ignore. Today, Tesla is arguing that it has no existing franchised dealers with which its factory sales compete, and that it is selling\n",
      "04/19/2022 17:37:31 - INFO - Summarization - Decoded labels: New Jersey's new law makes it illegal for Tesla to sell cars directly to consumers. John O'Dell: Tesla is not about to upend the franchised dealership system. He says as selling and servicing cars is an entirely different business from making them. O'Dell: As Tesla grows, it will also need to use the traditional dealership model.\n",
      "04/19/2022 17:37:31 - INFO - Summarization - \n",
      "\n",
      "04/19/2022 17:37:31 - INFO - Summarization - ***** Running training *****\n",
      "04/19/2022 17:37:31 - INFO - Summarization -   Num examples = 100\n",
      "04/19/2022 17:37:31 - INFO - Summarization -   Num Epochs = 1\n",
      "04/19/2022 17:37:31 - INFO - Summarization -   Total optimization steps = 13\n",
      "04/19/2022 17:37:51 - INFO - Summarization - Saving model checkpoint to output_dir/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)), 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)), 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)), 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}\n",
      "\n",
      "\n",
      "\n",
      "Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "\n",
      "\n",
      "\n",
      "Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "\n",
      "\n",
      "\n",
      "Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "\n",
      "\n",
      "\n",
      "Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "\n",
      "\n",
      "\n",
      "{'eval/rouge1/precision': 1.0, 'eval/rouge1/recall': 1.0, 'eval/rouge1/f1-score': 1.0, 'eval/rouge2/precision': 1.0, 'eval/rouge2/recall': 1.0, 'eval/rouge2/f1-score': 1.0, 'eval/rougeL/precision': 1.0, 'eval/rougeL/recall': 1.0, 'eval/rougeL/f1-score': 1.0, 'eval/rougeLsum/precision': 1.0, 'eval/rougeLsum/recall': 1.0, 'eval/rougeLsum/f1-score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2022 17:38:12 - INFO - Summarization - Saving model checkpoint to output_dir/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)), 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)), 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)), 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}\n",
      "\n",
      "\n",
      "\n",
      "Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "\n",
      "\n",
      "\n",
      "Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "\n",
      "\n",
      "\n",
      "Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "\n",
      "\n",
      "\n",
      "Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "\n",
      "\n",
      "\n",
      "{'eval/rouge1/precision': 1.0, 'eval/rouge1/recall': 1.0, 'eval/rouge1/f1-score': 1.0, 'eval/rouge2/precision': 1.0, 'eval/rouge2/recall': 1.0, 'eval/rouge2/f1-score': 1.0, 'eval/rougeL/precision': 1.0, 'eval/rougeL/recall': 1.0, 'eval/rougeL/f1-score': 1.0, 'eval/rougeLsum/precision': 1.0, 'eval/rougeLsum/recall': 1.0, 'eval/rougeLsum/f1-score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2022 17:38:26 - INFO - Summarization - Saving model checkpoint to output_dir/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)), 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)), 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)), 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}\n",
      "\n",
      "\n",
      "\n",
      "Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "\n",
      "\n",
      "\n",
      "Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "\n",
      "\n",
      "\n",
      "Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "\n",
      "\n",
      "\n",
      "Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
      "\n",
      "\n",
      "\n",
      "{'eval/rouge1/precision': 1.0, 'eval/rouge1/recall': 1.0, 'eval/rouge1/f1-score': 1.0, 'eval/rouge2/precision': 1.0, 'eval/rouge2/recall': 1.0, 'eval/rouge2/f1-score': 1.0, 'eval/rougeL/precision': 1.0, 'eval/rougeL/recall': 1.0, 'eval/rougeL/f1-score': 1.0, 'eval/rougeLsum/precision': 1.0, 'eval/rougeLsum/recall': 1.0, 'eval/rougeLsum/f1-score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:56<00:00,  4.32s/it]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "    if version.parse(datasets.__version__) < version.parse(\"1.18.0\"):\n",
    "        raise RuntimeError(\"This script requires Datasets 1.18.0 or higher. Please update via pip install -U datasets.\")\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78a98554bd959fe647588642884065a24bb8267d1904c1c950aa6b68cc3632ad"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('nlp_class')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
