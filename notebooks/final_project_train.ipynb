{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project - Gregory LeMasurier and Mojtaba Talaei Khoei\n",
    "\n",
    "Making the training file a jupyter notebook for the time being so I can easily debug it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (0.0.4)\n",
      "Requirement already satisfied: nltk in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (3.7)\n",
      "Requirement already satisfied: sentencepiece in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (0.1.96)\n",
      "Requirement already satisfied: absl-py in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from rouge-score) (1.0.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: numpy in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from rouge-score) (1.21.5)\n",
      "Requirement already satisfied: joblib in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from nltk) (2022.1.18)\n",
      "Requirement already satisfied: importlib-metadata in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from click->nltk) (4.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/greg/miniconda3/envs/nlp_class/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.7.0)\n"
     ]
    }
   ],
   "source": [
    "# Install Dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install rouge-score nltk sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Common Imports\n",
    "import os\n",
    "import random\n",
    "\n",
    "import transformers\n",
    "from transformers import PegasusTokenizer, PegasusConfig\n",
    "from transformers import PegasusForConditionalGeneration\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import wandb\n",
    "from packaging import version\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from transformer_mt import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"Summarization\")\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "datasets.utils.logging.set_verbosity_warning()\n",
    "transformers.utils.logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE Metric\n",
    "rouge = datasets.load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cnn_dailymail'\n",
    "dataset_version = '3.0.0'\n",
    "wandb_project = \"PegasusSummarization\"\n",
    "output_dir = \"output_dir/\"\n",
    "device = 'cpu'#'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = 'google/pegasus-xsum' \n",
    "tokenizer_name = 'google/pegasus-cnn_dailymail'\n",
    "seq_len = 512\n",
    "batch_size = 8\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 0.0\n",
    "num_train_epochs = 1\n",
    "lr_scheduler_type = \"linear\"\n",
    "num_warmup_steps = 0\n",
    "\n",
    "# Flag to make \n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logger.info(f\"Starting tokenizer training\")\n",
    "\n",
    "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "    logger.info(f\"Loading dataset\")\n",
    "\n",
    "    #wandb.init(project=wandb_project) #Skipping config for now - will add back later\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    raw_datasets = load_dataset(dataset_name, dataset_version)\n",
    "\n",
    "    # Make a small dataset for proof of concept\n",
    "    if debug:\n",
    "        raw_datasets = utils.sample_small_debug_dataset(raw_datasets)\n",
    "\n",
    "    ## TOKENIZER\n",
    "    #tokenizer_path = os.path.join(output_dir, f\"tokenizer\")\n",
    "    tokenizer = transformers.PegasusTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    ## PRETRAINED MODEL\n",
    "    #The pegasus model is too large to test on a laptop, so load a small config for now\n",
    "    #model = PegasusForConditionalGeneration(model_name).to(device)\n",
    "    config = PegasusConfig(\n",
    "            encoder_layers=2, \n",
    "            decoder_layers=2, \n",
    "            encoder_attention_heads=8, \n",
    "            decoder_attention_heads=8, \n",
    "            decoder_ffn_dim=1024, \n",
    "            encoder_ffn_dim=1024,\n",
    "            max_position_embeddings=seq_len,\n",
    "            )\n",
    "    model = PegasusForConditionalGeneration(config).to(device)\n",
    "\n",
    "\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        inputs = [ex for ex in examples['article']]\n",
    "        targets = [ex for ex in examples['highlights']]\n",
    "        model_inputs = tokenizer(inputs, max_length=seq_len, truncation=True)\n",
    "        model_inputs['labels'] = tokenizer(targets, max_length=seq_len, truncation=True)['input_ids']\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=8,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Tokenizing the dataset\",\n",
    "    )\n",
    "\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"validation\"] if \"validaion\" in tokenized_datasets else tokenized_datasets[\"test\"]\n",
    "\n",
    "    for index in random.sample(range(len(train_dataset)), 2):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "        logger.info(f\"Decoded input_ids: {tokenizer.decode(train_dataset[index]['input_ids'])}\")\n",
    "        logger.info(f\"Decoded labels: {tokenizer.decode(train_dataset[index]['labels'])}\")\n",
    "        logger.info(\"\\n\")\n",
    "\n",
    "    #collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer, max_length=seq_len, padding='max_length')\n",
    "    collator = transformers.DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, max_length=seq_len, padding='max_length')\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        shuffle=True, \n",
    "        collate_fn=collator, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, \n",
    "        collate_fn=collator, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "    \n",
    "    # Scheduler and math around the number of training steps.\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = transformers.get_scheduler(\n",
    "        name=lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_train_steps,\n",
    "    )\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {num_train_epochs}\")\n",
    "    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n",
    "    #progress_bar = tqdm(range(max_train_steps))\n",
    "\n",
    "    # Log a pre-processed training example to make sure the pre-processing does not have bugs in it\n",
    "    # and we do not input garbage to our model.\n",
    "    batch = next(iter(train_dataloader))\n",
    "    #print(len(batch['input_ids']))\n",
    "    #print(len(batch['labels']))\n",
    "    #logger.info(\"Look at the data that we input into the model, check that it looks like what we expect.\")\n",
    "    #for index in random.sample(range(len(batch)), 2):\n",
    "    #    logger.info(f\"Decoded input_ids size: {len(batch['input_ids'][index])}\")\n",
    "    #    logger.info(f\"Decoded input_ids: {tokenizer.decode(batch['input_ids'][index])}\")\n",
    "    #    logger.info(f\"Decoded labels size: {len(batch['labels'][index])}\")\n",
    "    #    logger.info(f\"Decoded labels: {tokenizer.decode(batch['labels'][index])}\")\n",
    "    #    logger.info(\"\\n\")\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(num_train_epochs):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            #print(\"EPOCH: \" + str(epoch) + \" BATCH: \" + str(batch))\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            print(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/19/2022 09:16:47 - INFO - Summarization - Starting tokenizer training\n",
      "04/19/2022 09:16:47 - INFO - Summarization - Loading dataset\n",
      "04/19/2022 09:16:47 - WARNING - datasets.builder - Reusing dataset cnn_dailymail (/home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "100%|██████████| 3/3 [00:00<00:00, 605.56it/s]\n",
      "Tokenizing the dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tokenizing the dataset #0: 100%|██████████| 1/1 [00:00<00:00, 10.93ba/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Tokenizing the dataset #2: 100%|██████████| 1/1 [00:00<00:00,  6.97ba/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Tokenizing the dataset #6: 100%|██████████| 1/1 [00:00<00:00,  7.23ba/s]\n",
      "Tokenizing the dataset #4: 100%|██████████| 1/1 [00:00<00:00,  6.92ba/s]\n",
      "\n",
      "\n",
      "Tokenizing the dataset #3: 100%|██████████| 1/1 [00:00<00:00,  6.04ba/s]\n",
      "Tokenizing the dataset #1: 100%|██████████| 1/1 [00:00<00:00,  5.16ba/s]\n",
      "Tokenizing the dataset #7: 100%|██████████| 1/1 [00:00<00:00, 12.37ba/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tokenizing the dataset #5: 100%|██████████| 1/1 [00:00<00:00,  7.44ba/s]\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-b4e46c2133886c46.arrow\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-05cff293938459ce.arrow\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-9e8ba141f621590d.arrow\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-be103723a45c1233.arrow\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-07280f2b73265d9f.arrow\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-4e129ef0a6b498f1.arrow\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-8e08f0d05e6eb305.arrow\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-106ef055002264df.arrow\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-b4e46c2133886c46.arrow\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-05cff293938459ce.arrow\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-9e8ba141f621590d.arrow\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-be103723a45c1233.arrow\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-07280f2b73265d9f.arrow\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-4e129ef0a6b498f1.arrow\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-8e08f0d05e6eb305.arrow\n",
      "04/19/2022 09:16:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/greg/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-106ef055002264df.arrow\n",
      "04/19/2022 09:16:56 - INFO - Summarization - Sample 75 of the training set: {'input_ids': [168, 117, 114, 2822, 233, 111, 13537, 233, 587, 113, 1247, 134, 201, 107, 202, 1512, 4347, 19966, 27150, 116, 124, 142, 36951, 70414, 8733, 108, 29459, 115, 203, 33579, 115, 111, 6207, 112, 6329, 203, 14552, 429, 107, 507, 1055, 108, 5340, 141, 12719, 40921, 115, 114, 1247, 4722, 109, 2789, 61006, 44343, 1322, 113, 16321, 3260, 108, 403, 114, 1533, 52193, 2281, 162, 6659, 109, 19966, 3175, 142, 2517, 500, 488, 203, 282, 628, 107, 25953, 34909, 151, 182, 4347, 19966, 23196, 116, 115, 111, 13396, 203, 33579, 190, 114, 70414, 8733, 110, 107, 21910, 115, 151, 139, 1512, 4209, 2171, 112, 3811, 109, 51568, 2517, 135, 109, 1230, 110, 107, 13576, 121, 2467, 151, 139, 19966, 16607, 203, 7492, 269, 8021, 109, 8733, 135, 109, 1230, 110, 107, 2113, 109, 4347, 19966, 117, 18001, 110, 107, 118, 203, 4964, 29277, 108, 111, 133, 174, 2908, 12097, 62015, 111, 254, 114, 110, 107, 758, 4276, 108, 1497, 697, 136, 117, 109, 211, 9355, 2281, 124, 114, 110, 107, 8733, 107, 139, 339, 2005, 403, 188, 228, 2286, 113, 918, 108, 111, 195, 784, 11586, 141, 4182, 9023, 109, 15831, 46659, 18572, 115, 5608, 139, 1297, 108, 111, 114, 3180, 113, 109, 455, 108, 195, 1299, 115, 136, 625, 131, 116, 3118, 113, 60156, 1810, 107, 9625, 17130, 2858, 108, 113, 109, 76305, 2377, 113, 1169, 108, 374, 109, 3901, 333, 114, 3337, 882, 553, 107, 721, 9866, 109, 16412, 5437, 108, 5449, 17130, 2858, 111, 215, 4134, 374, 114, 51393, 162, 157, 3812, 112, 129, 120, 113, 109, 11289, 8733, 188, 4280, 429, 107, 452, 243, 151, 1034, 362, 131, 116, 109, 211, 166, 125, 131, 261, 684, 742, 172, 126, 107, 168, 131, 116, 2822, 118, 4347, 32341, 112, 110, 116, 61463, 164, 8733, 107, 1034, 187, 110, 107, 1148, 109, 8733, 51393, 211, 130, 125, 8270, 109, 8964, 124, 114, 3337, 553, 110, 107, 112, 2514, 165, 1959, 1519, 111, 411, 5545, 108, 155, 364, 1373, 110, 107, 1401, 160, 126, 107, 1034, 621, 195, 220, 423, 78009, 3815, 115, 109, 2505, 108, 111, 126, 1740, 172, 109, 8733, 196, 174, 850, 111, 237, 188, 3135, 111, 2342, 107, 8555, 151, 222, 136, 1785, 109, 19966, 493, 471, 122, 109, 8733, 110, 107, 24606, 151, 139, 4209, 137, 237, 129, 684, 11745, 203, 33579, 115, 111, 1215, 112, 6329, 203, 14552, 429, 110, 107, 72197, 151, 139, 19966, 237, 6329, 116, 109, 8733, 165, 113, 1785, 112, 193, 203, 2276, 110, 107, 1034, 362, 140, 209, 244, 145, 419, 247, 112, 110, 107, 3082, 120, 125, 4120, 109, 1055, 135, 109, 1529, 111, 1038, 252, 579, 110, 107, 424, 107, 125, 1826, 131, 144, 697, 180, 125, 140, 1749, 107, 125, 131, 261, 174, 11815, 8733, 2791, 113, 1323, 115, 3260, 118, 1204, 231, 108, 136, 117, 109, 211, 166, 125, 131, 261, 684, 742, 172, 136, 107, 131, 982, 110, 107, 8352, 63913, 24371, 108, 135, 109, 10694, 10207, 2377, 233, 162, 659, 110, 107, 122, 109, 76305, 412, 5342, 2922, 233, 243, 151, 1034, 159, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1328, 46412, 4209, 113, 14552, 3613, 36951, 14552, 135, 893, 110, 107, 1249, 6329, 116, 109, 23589, 14002, 429, 112, 129, 9213, 110, 107, 8306, 3901, 195, 5340, 141, 4182, 115, 61006, 44343, 108, 3260, 110, 107, 14783, 27700, 697, 126, 117, 109, 211, 166, 253, 142, 2281, 148, 174, 14124, 110, 107, 1]}.\n",
      "04/19/2022 09:16:57 - INFO - Summarization - Decoded input_ids: It is a rare - and brutal - example of nature at work. A powerful golden eagle swoops on an unsuspecting Sika deer, digs in its claws in and tries to drag its prey away. These images, captured by zoologists in a nature reserve the remote Lazovsky region of southeast Russia, show a particularly audacious attack which sees the eagle fell an animal several times its own size. Swoop: This golden eagle soars in and sinks its claws into a Sika deer. Dig in: The powerful bird starts to lift the panicked animal from the ground. Lift-off: The eagle spreads its wings before lifting the deer from the ground. Although the golden eagle is notorious. for its bold hunts, and have been caught attacking coyote and even a. young bear, experts believe this is the first documented attack on a. deer. The three frame show just two seconds of action, and were taken accidentally by scientists researching the endangered Siberian tiger in 2011. The pictures, and a description of the event, were published in this month's Journal of Raptor Research. Linda Kerley, of the Zoological Society of London, found the shots during a routine equipment check. After discovering the astonishing photograph, Ms Kerley and her colleagues found a carcass which they believed to be that of the unfortunate deer just yards away. She said: 'It's the first time I've seen anything like it. It's rare for golden eagles to snatch up deer. 'I. saw the deer carcass first as I approached the trap on a routine check. to switch out memory cards and change batteries, but something felt. wrong about it. 'There were no large carnivore tracks in the snow, and it looked like the deer had been running and then just stopped and died. Impact: In this shot the eagle makes contact with the deer. Drag: The bird can then be seen digging its claws in and starting to drag its prey away. Endgame: The eagle then drags the deer out of shot to make its meal. 'It was only after we got back to. camp that I checked the images from the camera and pieced everything. together. I couldn't believe what I was seeing. I've been assessing deer causes of death in Russia for 18 years, this is the first time I've seen anything like this.' Dr. Jonathan Slaght, from the Wildlife Conservation Society - which works. with the Zoological Soceity - said: 'The\n",
      "04/19/2022 09:16:57 - INFO - Summarization - Decoded labels: Vicious bird of prey attacks unsuspecting prey from behind. Then drags the terrified beast away to be eaten. Amazing shots were captured by scientists in Lazovsky, Russia. Zoologists believe it is the first time such an attack has been photographed.\n",
      "04/19/2022 09:16:57 - INFO - Summarization - \n",
      "\n",
      "04/19/2022 09:16:57 - INFO - Summarization - Sample 24 of the training set: {'input_ids': [30813, 58736, 42480, 114, 2631, 115, 6052, 111, 3040, 339, 1564, 3584, 269, 9397, 646, 6762, 113, 392, 30926, 115, 114, 57385, 107, 139, 7355, 1024, 108, 6834, 115, 810, 108, 42480, 110, 107, 190, 1637, 134, 983, 4839, 502, 108, 3381, 113, 29260, 108, 111, 898, 392, 112, 753, 414, 269, 157, 538, 183, 275, 110, 107, 645, 114, 1281, 299, 107, 139, 2281, 642, 130, 6282, 37011, 111, 176, 1857, 121, 20812, 54533, 133, 886, 972, 113, 109, 2906, 12304, 1863, 113, 21571, 5106, 111, 109, 517, 113, 90405, 381, 1408, 10754, 4220, 16885, 317, 30621, 10505, 111, 109, 45315, 121, 4105, 657, 107, 1041, 51628, 111, 3785, 52214, 116, 801, 317, 109, 24329, 111, 657, 121, 35935, 316, 3062, 108, 109, 399, 4318, 148, 174, 518, 1600, 277, 15310, 15307, 109, 3459, 110, 107, 18491, 151, 139, 58736, 42480, 983, 4839, 502, 108, 3381, 113, 29260, 108, 136, 1039, 108, 646, 6762, 113, 392, 30926, 115, 114, 57385, 110, 107, 983, 2474, 40450, 45811, 456, 606, 130, 109, 6282, 816, 113, 6052, 111, 109, 55547, 143, 187, 54854, 158, 111, 176, 30621, 121, 4105, 24329, 133, 3922, 972, 113, 983, 4839, 6622, 108, 330, 109, 517, 113, 90405, 111, 972, 113, 21571, 5106, 108, 381, 1095, 1408, 107, 6052, 117, 767, 42518, 122, 203, 3741, 12237, 115, 3459, 381, 109, 46569, 64918, 115, 4822, 111, 9248, 173, 109, 531, 140, 5817, 112, 109, 27419, 113, 3541, 1795, 2409, 109, 2210, 113, 11371, 113, 1873, 113, 787, 7736, 107, 139, 807, 3459, 148, 174, 20527, 141, 30621, 6452, 7630, 134, 109, 45315, 121, 4105, 657, 115, 29260, 108, 130, 210, 130, 109, 3541, 1795, 115, 12863, 6881, 107, 73041, 148, 2777, 165, 4198, 113, 10011, 3613, 124, 302, 2600, 113, 109, 3660, 111, 10253, 114, 13537, 515, 113, 6282, 2613, 115, 13853, 365, 203, 562, 107, 168, 140, 146, 1501, 786, 175, 73041, 140, 893, 109, 2631, 2281, 107, 21488, 148, 4620, 109, 965, 113, 110, 39323, 110, 107, 55768, 115, 913, 108, 109, 1330, 2357, 1323, 112, 267, 167, 571, 136, 232, 107, 139, 19350, 31871, 872, 148, 37431, 109, 11340, 1628, 109, 110, 107, 657, 4121, 130, 126, 8293, 112, 2512, 114, 12237, 115, 46569, 110, 107, 3459, 107, 47615, 116, 151, 54274, 45413, 109, 378, 113, 114, 439, 9517, 2281, 1678, 136, 396, 108, 297, 113, 114, 61666, 113, 3459, 162, 1148, 110, 39323, 200, 3040, 115, 913, 110, 107, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [60060, 127, 270, 886, 115, 57385, 134, 983, 4839, 502, 3381, 113, 29260, 110, 107, 30344, 416, 2281, 828, 12304, 1863, 108, 21571, 5106, 108, 635, 295, 136, 1039, 110, 107, 7556, 117, 270, 886, 141, 6282, 37011, 111, 1857, 121, 20812, 29392, 110, 107, 16502, 6194, 200, 2342, 289, 625, 115, 531, 131, 116, 3741, 3459, 381, 4001, 110, 107, 1]}.\n",
      "04/19/2022 09:16:57 - INFO - Summarization - Decoded input_ids: Islamist gunmen stormed a university in Iraq and killed three police officers before briefly taking dozens of students hostage in a dormitory. The armed men, dressed in black, stormed. into rooms at Anbar University, west of Baghdad, and told students to stay put before they let them go. following a stand off. The attack came as Islamic extremists and other anti-government militias have held parts of the nearby provincial capital of Ramadi and the city of Fallujah since December amid rising tensions between Sunni Muslims and the Shiite-led government. While shelling and gunbattles continue between the militants and government-allied forces, the school largely has been left alone while civilians fled the violence. Attack: The gunmen stormed Anbar University, west of Baghdad, this morning, taking dozens of students hostage in a dormitory. An Al Qaeda splinter group known as the Islamic State of Iraq and the Levant (ISIL) and other Sunni-led militants have controlled parts of Anbar province, including the city of Fallujah and parts of Ramadi, since late December. Iraq is currently grappling with its worst surge in violence since the sectarian bloodshed in 2006 and 2007, when the country was pushed to the brink of civil war despite the presence of tens of thousands of US troops. The latest violence has been fueled by Sunni Muslim anger at the Shiite-led government in Baghdad, as well as the civil war in neighboring Syria. ISIL has carried out scores of deadly attacks on both sides of the border and imposed a brutal form of Islamic rule in territories under its control. It was not immediately clear if ISIL was behind the university attack. Violence has claimed the lives of 799. Iraqis in May, the highest monthly death toll so far this year. The soaring casualty rate has underlined the daunting challenges the. government faces as it struggles to contain a surge in sectarian. violence. Tensions: Firefighters extinguish the site of a car bomb attack earlier this week, part of a spate of violence which saw 799 people killed in May.\n",
      "04/19/2022 09:16:57 - INFO - Summarization - Decoded labels: Dozens are being held in dormitory at Anbar University west of Baghdad. Officials say attack near provincial capital, Ramadi, took place this morning. Region is being held by Islamic extremists and anti-government factions. Nearly 800 people died last month in country's worst violence since 2007.\n",
      "04/19/2022 09:16:57 - INFO - Summarization - \n",
      "\n",
      "04/19/2022 09:16:57 - INFO - Summarization - ***** Running training *****\n",
      "04/19/2022 09:16:57 - INFO - Summarization -   Num examples = 100\n",
      "04/19/2022 09:16:57 - INFO - Summarization -   Num Epochs = 1\n",
      "04/19/2022 09:16:57 - INFO - Summarization -   Total optimization steps = 13\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b9fb469b2a2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1.18.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This script requires Datasets 1.18.0 or higher. Please update via pip install -U datasets.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-dabe890075bc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             )\n\u001b[1;32m    128\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp_class/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp_class/lib/python3.7/site-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1392\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1394\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m         )\n\u001b[1;32m   1396\u001b[0m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp_class/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp_class/lib/python3.7/site-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1228\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m             )\n\u001b[1;32m   1232\u001b[0m         \u001b[0;31m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp_class/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp_class/lib/python3.7/site-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp_class/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp_class/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp_class/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "    if version.parse(datasets.__version__) < version.parse(\"1.18.0\"):\n",
    "        raise RuntimeError(\"This script requires Datasets 1.18.0 or higher. Please update via pip install -U datasets.\")\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78a98554bd959fe647588642884065a24bb8267d1904c1c950aa6b68cc3632ad"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('nlp_class')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
